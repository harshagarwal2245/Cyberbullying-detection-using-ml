{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f083d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f07c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205af2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs\n",
    "df=pd.read_json(\"D:\\B.E\\sem7\\Project work stage1\\cyberbullying\\Dataset for Detection of Cyber-Trolls.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb17b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0                             Get fucking real dude.   \n",
       "1   She is as dirty as they come  and that crook ...   \n",
       "2   why did you fuck it up. I could do it all day...   \n",
       "3   Dude they dont finish enclosing the fucking s...   \n",
       "4   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "                      annotation  extras  label  \n",
       "0  {'notes': '', 'label': ['1']}     NaN      1  \n",
       "1  {'notes': '', 'label': ['1']}     NaN      1  \n",
       "2  {'notes': '', 'label': ['1']}     NaN      1  \n",
       "3  {'notes': '', 'label': ['1']}     NaN      1  \n",
       "4  {'notes': '', 'label': ['1']}     NaN      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"] = df.annotation.apply(lambda x: x.get('label'))\n",
    "df[\"label\"] = df.label.apply(lambda x: x[0])\n",
    "df['label']=df['label'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9745db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get fucking real dude.</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She is as dirty as they come  and that crook ...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why did you fuck it up. I could do it all day...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dude they dont finish enclosing the fucking s...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WTF are you talking about Men? No men thats n...</td>\n",
       "      <td>{'notes': '', 'label': ['1']}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0                             Get fucking real dude.   \n",
       "1   She is as dirty as they come  and that crook ...   \n",
       "2   why did you fuck it up. I could do it all day...   \n",
       "3   Dude they dont finish enclosing the fucking s...   \n",
       "4   WTF are you talking about Men? No men thats n...   \n",
       "\n",
       "                      annotation  label  \n",
       "0  {'notes': '', 'label': ['1']}      1  \n",
       "1  {'notes': '', 'label': ['1']}      1  \n",
       "2  {'notes': '', 'label': ['1']}      1  \n",
       "3  {'notes': '', 'label': ['1']}      1  \n",
       "4  {'notes': '', 'label': ['1']}      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['extras'],inplace=True,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b263fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[\"content\"]\n",
    "y=df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e96648",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44a96f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a661e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 12140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'hate': 3,\n",
       " 'know': 4,\n",
       " 'french': 5,\n",
       " '...': 6,\n",
       " 'use': 7,\n",
       " 'fuck': 8,\n",
       " 'shit': 9,\n",
       " 'even': 10,\n",
       " 'right': 11,\n",
       " 'older': 12,\n",
       " 'men': 13,\n",
       " 'hit': 14,\n",
       " 'forget': 15,\n",
       " 'extra': 16,\n",
       " '2': 17,\n",
       " 'liter': 18,\n",
       " 'gotta': 19,\n",
       " 'dunno': 20,\n",
       " 'saw': 21,\n",
       " 'go': 22,\n",
       " 'top': 23,\n",
       " 'stair': 24,\n",
       " 'seem': 25,\n",
       " 'like': 26,\n",
       " 'ass': 27,\n",
       " 'slide': 28,\n",
       " 'clue': 29,\n",
       " \"i'v\": 30,\n",
       " 'stumbl': 31,\n",
       " 'upon': 32,\n",
       " 'laugh': 33,\n",
       " 'might': 34,\n",
       " 'yeah': 35,\n",
       " 'suck': 36,\n",
       " '6': 37,\n",
       " 'yr': 38,\n",
       " 'realiz': 39,\n",
       " 'dont': 40,\n",
       " 'get': 41,\n",
       " 'snow': 42,\n",
       " 'ice': 43,\n",
       " 'nobodi': 44,\n",
       " 'drive': 45,\n",
       " 'rain': 46,\n",
       " 'let': 47,\n",
       " 'alon': 48,\n",
       " 'bitch': 49,\n",
       " 'craaazzzi': 50,\n",
       " 'p': 51,\n",
       " ':)': 52,\n",
       " 'peopl': 53,\n",
       " 'word': 54,\n",
       " 'bother': 55,\n",
       " 'constant': 56,\n",
       " 'receiv': 57,\n",
       " 'giver': 58,\n",
       " 'make': 59,\n",
       " 'prepar': 60,\n",
       " 'prison': 61,\n",
       " 'blagojevich': 62,\n",
       " 'new': 63,\n",
       " 'campaign': 64,\n",
       " 'slogan': 65,\n",
       " 'rhyme': 66,\n",
       " 'damnit': 67,\n",
       " 'start': 68,\n",
       " 'short': 69,\n",
       " 'stori': 70,\n",
       " 'dentist': 71,\n",
       " 'flight': 72,\n",
       " 'delay': 73,\n",
       " 'hour': 74,\n",
       " 'termin': 75,\n",
       " 'ef': 76,\n",
       " 'washington': 77,\n",
       " 'pleas': 78,\n",
       " 'explain': 79,\n",
       " 'everi': 80,\n",
       " 'time': 81,\n",
       " 'say': 82,\n",
       " 'someth': 83,\n",
       " 'piss': 84,\n",
       " 'fall': 85,\n",
       " 'come': 86,\n",
       " 'server-hugg': 87,\n",
       " 'monik': 88,\n",
       " 'wise-ass': 89,\n",
       " 'hope': 90,\n",
       " 'stuck': 91,\n",
       " 'long': 92,\n",
       " 'funni': 93,\n",
       " 'two': 94,\n",
       " 'second': 95,\n",
       " 'ago': 96,\n",
       " 'somebodi': 97,\n",
       " 'walk': 98,\n",
       " 'offic': 99,\n",
       " 'said': 100,\n",
       " 'winter': 101,\n",
       " 'lol': 102,\n",
       " 'gay': 103,\n",
       " 'question': 104,\n",
       " 'sexual': 105,\n",
       " 'highschool': 106,\n",
       " 'mayb': 107,\n",
       " 'haha': 108,\n",
       " 'yu': 109,\n",
       " 'tld': 110,\n",
       " 'lmfao': 111,\n",
       " 'lykk': 112,\n",
       " 'yestadayi': 113,\n",
       " 'stole': 114,\n",
       " 'downer': 115,\n",
       " \"there'\": 116,\n",
       " 'chanc': 117,\n",
       " 'may': 118,\n",
       " 'releas': 119,\n",
       " 'mean': 120,\n",
       " 'young': 121,\n",
       " 'kaia': 122,\n",
       " 'either': 123,\n",
       " 'grow': 124,\n",
       " 'greatest': 125,\n",
       " 'femal': 126,\n",
       " 'beatmak': 127,\n",
       " 'ever': 128,\n",
       " 'rap': 129,\n",
       " 'legend': 130,\n",
       " 'sever': 131,\n",
       " 'cuss': 132,\n",
       " 'ye': 133,\n",
       " 'sing': 134,\n",
       " 'along': 135,\n",
       " \"i'd\": 136,\n",
       " 'think': 137,\n",
       " \"i'm\": 138,\n",
       " 'tortur': 139,\n",
       " 'poor': 140,\n",
       " 'littl': 141,\n",
       " 'pixel': 142,\n",
       " 'someon': 143,\n",
       " 'puke': 144,\n",
       " 'ear': 145,\n",
       " 'happi': 146,\n",
       " 'birthday': 147,\n",
       " 'damn': 148,\n",
       " 'sexi': 149,\n",
       " 'hobo': 150,\n",
       " 'ya': 151,\n",
       " 'skippi': 152,\n",
       " 'dream': 153,\n",
       " 'car': 154,\n",
       " 'want': 155,\n",
       " 'visit': 156,\n",
       " 'countri': 157,\n",
       " 'allow': 158,\n",
       " 'passport': 159,\n",
       " 'smile': 160,\n",
       " 'cabo': 161,\n",
       " 'soon': 162,\n",
       " 'hmm': 163,\n",
       " 'wonderr': 164,\n",
       " 'maybe': 165,\n",
       " 'babeyyboy': 166,\n",
       " '<3': 167,\n",
       " 'aw': 168,\n",
       " 'emo': 169,\n",
       " 'week': 170,\n",
       " 'lot': 171,\n",
       " 'level': 172,\n",
       " 'ha': 173,\n",
       " \"let'\": 174,\n",
       " 'fun': 175,\n",
       " 'tonight': 176,\n",
       " 'peski': 177,\n",
       " 'pirat': 178,\n",
       " 'joke': 179,\n",
       " 'man': 180,\n",
       " '..': 181,\n",
       " 'wasnt': 182,\n",
       " 'take': 183,\n",
       " 'anyth': 184,\n",
       " 'd:': 185,\n",
       " 'well': 186,\n",
       " 'favorit': 187,\n",
       " 'thing': 188,\n",
       " 'weekend': 189,\n",
       " 'yea': 190,\n",
       " 'hv': 191,\n",
       " 'admit': 192,\n",
       " 'post': 193,\n",
       " 'reread': 194,\n",
       " 'thought': 195,\n",
       " 'thng': 196,\n",
       " 'guess': 197,\n",
       " 'feelin': 198,\n",
       " 'si': 199,\n",
       " 'love': 200,\n",
       " 'got': 201,\n",
       " 'alittl': 202,\n",
       " 'yall': 203,\n",
       " 'r': 204,\n",
       " 'u': 205,\n",
       " 'drti': 206,\n",
       " 'slut': 207,\n",
       " 'wutchu': 208,\n",
       " 'tellin': 209,\n",
       " 'ohh': 210,\n",
       " 'um': 211,\n",
       " 'one': 212,\n",
       " 'realli': 213,\n",
       " 'hard': 214,\n",
       " 'spose': 215,\n",
       " 'andi': 216,\n",
       " 'bradi': 217,\n",
       " 'would': 218,\n",
       " 'choos': 219,\n",
       " 'choic': 220,\n",
       " '=]': 221,\n",
       " 'shaun': 222,\n",
       " 'nurs': 223,\n",
       " 'degre': 224,\n",
       " 'alreadi': 225,\n",
       " 'lott': 226,\n",
       " 'land': 227,\n",
       " 'live': 228,\n",
       " 'money': 229,\n",
       " 'could': 230,\n",
       " 'give': 231,\n",
       " 'away': 232,\n",
       " 'work': 233,\n",
       " 'entir': 234,\n",
       " 'life': 235,\n",
       " 'never': 236,\n",
       " 'ahead': 237,\n",
       " 'dog': 238,\n",
       " 'bullmastiff': 239,\n",
       " 'purebre': 240,\n",
       " 'see': 241,\n",
       " 'listen': 242,\n",
       " 'cow-lov': 243,\n",
       " 'yahoo': 244,\n",
       " 'put': 245,\n",
       " 'slurpe': 246,\n",
       " 'bag': 247,\n",
       " 'cup': 248,\n",
       " 'need': 249,\n",
       " 'la': 250,\n",
       " 'end': 251,\n",
       " 'month': 252,\n",
       " ':(': 253,\n",
       " 'standard': 254,\n",
       " 'twitter': 255,\n",
       " 'site': 256,\n",
       " 'tough': 257,\n",
       " ':p': 258,\n",
       " 'find': 259,\n",
       " 'anoth': 260,\n",
       " 'addict': 261,\n",
       " 'believ': 262,\n",
       " 'earth': 263,\n",
       " 'actual': 264,\n",
       " 'quot': 265,\n",
       " 'thought-provok': 266,\n",
       " 'movi': 267,\n",
       " 'name': 268,\n",
       " 'donni': 269,\n",
       " 'darko': 270,\n",
       " '2012': 271,\n",
       " ';d': 272,\n",
       " 'hear': 273,\n",
       " 'transmiss': 274,\n",
       " 'friend': 275,\n",
       " 'total': 276,\n",
       " 'poser': 277,\n",
       " 'idr': 278,\n",
       " 'tho': 279,\n",
       " 'christof': 280,\n",
       " 'drew': 281,\n",
       " 'hahahar': 282,\n",
       " 'doubt': 283,\n",
       " 'password': 284,\n",
       " 'australian': 285,\n",
       " \"we'r\": 286,\n",
       " 'bad': 287,\n",
       " 'gather': 288,\n",
       " 'cours': 289,\n",
       " 'best': 290,\n",
       " 'part': 291,\n",
       " 'hang': 292,\n",
       " 'downtown': 293,\n",
       " 'chang': 294,\n",
       " 'role': 295,\n",
       " 'less': 296,\n",
       " 'hell': 297,\n",
       " 'still': 298,\n",
       " 'sting': 299,\n",
       " 'much': 300,\n",
       " 'barb': 301,\n",
       " 'wont': 302,\n",
       " 'pass': 303,\n",
       " 'pretti': 304,\n",
       " 'sure': 305,\n",
       " \"i'll\": 306,\n",
       " 'oh': 307,\n",
       " 'good': 308,\n",
       " 'day': 309,\n",
       " '19': 310,\n",
       " 'tab': 311,\n",
       " '1': 312,\n",
       " 'window': 313,\n",
       " 'meant': 314,\n",
       " 'readi': 315,\n",
       " 'wild': 316,\n",
       " 'freaki': 317,\n",
       " 'mind': 318,\n",
       " 'bore': 319,\n",
       " 'suppos': 320,\n",
       " 'collect': 321,\n",
       " 'beani': 322,\n",
       " 'babi': 323,\n",
       " 'younger': 324,\n",
       " 'case': 325,\n",
       " 'everyth': 326,\n",
       " 'wow': 327,\n",
       " 'ohkay': 328,\n",
       " 'christma': 329,\n",
       " 'kinda': 330,\n",
       " 'outta': 331,\n",
       " 'santa': 332,\n",
       " 'hat': 333,\n",
       " 'sound': 334,\n",
       " 'fourth': 335,\n",
       " 'meal': 336,\n",
       " 'zone': 337,\n",
       " 'usual': 338,\n",
       " 'sleep': 339,\n",
       " \"drew'\": 340,\n",
       " 'tini': 341,\n",
       " 'knee': 342,\n",
       " 'except': 343,\n",
       " 'late': 344,\n",
       " 'skip': 345,\n",
       " 'town': 346,\n",
       " 'express': 347,\n",
       " 'train': 348,\n",
       " \"that'\": 349,\n",
       " 'adventur': 350,\n",
       " 'present': 351,\n",
       " 'back': 352,\n",
       " 'gift': 353,\n",
       " 'given': 354,\n",
       " 'palm': 355,\n",
       " 'spring': 356,\n",
       " 'holiday': 357,\n",
       " 'figur': 358,\n",
       " 'steve': 359,\n",
       " 'violat': 360,\n",
       " 'truth': 361,\n",
       " 'must': 362,\n",
       " 'urself': 363,\n",
       " 'though': 364,\n",
       " 'product': 365,\n",
       " 'wish': 366,\n",
       " 'tuesday': 367,\n",
       " 'done': 368,\n",
       " 'yet': 369,\n",
       " 'religion': 370,\n",
       " 'lookit': 371,\n",
       " 'line': 372,\n",
       " 'answer': 373,\n",
       " 'biatchh': 374,\n",
       " 'l': 375,\n",
       " 'ashle': 376,\n",
       " 'stare': 377,\n",
       " 'face': 378,\n",
       " 'awhil': 379,\n",
       " 'weird': 380,\n",
       " 'creepi': 381,\n",
       " 'sort': 382,\n",
       " 'eat': 383,\n",
       " 'dinner': 384,\n",
       " 'person': 385,\n",
       " 'dead': 386,\n",
       " 'aliv': 387,\n",
       " 'goddamn': 388,\n",
       " 'kj': 389,\n",
       " '49': 390,\n",
       " 'alyssa': 391,\n",
       " 'milano': 392,\n",
       " 'tha': 393,\n",
       " 'market': 394,\n",
       " 'complet': 395,\n",
       " 'silenc': 396,\n",
       " 'dark': 397,\n",
       " 'ckuss': 398,\n",
       " 'yew': 399,\n",
       " 'forgot': 400,\n",
       " 'bout': 401,\n",
       " 'mhe': 402,\n",
       " 'tomorrow': 403,\n",
       " 'morn': 404,\n",
       " 'updat': 405,\n",
       " 'hungov': 406,\n",
       " 'yooozer': 407,\n",
       " '20': 408,\n",
       " '442': 409,\n",
       " 'homi': 410,\n",
       " '6:30': 411,\n",
       " 'pm': 412,\n",
       " 'friday': 413,\n",
       " 'what': 414,\n",
       " 'candyman': 415,\n",
       " 'hahha': 416,\n",
       " 'comput': 417,\n",
       " 'mobil': 418,\n",
       " 'phone': 419,\n",
       " 'cute': 420,\n",
       " 'video': 421,\n",
       " 'lexi': 422,\n",
       " 'hve': 423,\n",
       " 'weridest': 424,\n",
       " 'cutest': 425,\n",
       " 'toy': 426,\n",
       " 'wht': 427,\n",
       " \"u'll\": 428,\n",
       " 'shut': 429,\n",
       " 'ach': 430,\n",
       " 'mil': 431,\n",
       " 'brave': 432,\n",
       " 'woman': 433,\n",
       " 'place': 434,\n",
       " 'glad': 435,\n",
       " 'boy': 436,\n",
       " 'old': 437,\n",
       " 'shudder': 438,\n",
       " 'lie': 439,\n",
       " 'everywher': 440,\n",
       " 'myspac': 441,\n",
       " 'page': 442,\n",
       " 'caught': 443,\n",
       " 'earli': 444,\n",
       " 'embryo': 445,\n",
       " 'differ': 446,\n",
       " 'guy': 447,\n",
       " 'blow': 448,\n",
       " 'load': 449,\n",
       " 'kill': 450,\n",
       " 'sperm': 451,\n",
       " 'innoc': 452,\n",
       " 'human': 453,\n",
       " 'although': 454,\n",
       " 'mani': 455,\n",
       " 'taken': 456,\n",
       " 'war': 457,\n",
       " 'sicken': 458,\n",
       " 'ah': 459,\n",
       " 'traffic': 460,\n",
       " 'comic': 461,\n",
       " 'strip': 462,\n",
       " 'call': 463,\n",
       " 'bc': 464,\n",
       " 'real': 465,\n",
       " 'paper': 466,\n",
       " 'cool': 467,\n",
       " 'ok': 468,\n",
       " 'commut': 469,\n",
       " 'brush': 470,\n",
       " 'knot': 471,\n",
       " 'hair.id': 472,\n",
       " 'rather': 473,\n",
       " 'polish': 474,\n",
       " 'occassionallybi': 475,\n",
       " 'airport': 476,\n",
       " 'shoe': 477,\n",
       " 'shine': 478,\n",
       " 'song': 479,\n",
       " 'danc': 480,\n",
       " 'gold': 481,\n",
       " 'upgrad': 482,\n",
       " 'bling': 483,\n",
       " 'mouth': 484,\n",
       " 'worst': 485,\n",
       " 'commerci': 486,\n",
       " 'watch': 487,\n",
       " 'elf': 488,\n",
       " 'instead': 489,\n",
       " 'drunk': 490,\n",
       " 'floor': 491,\n",
       " 'youu': 492,\n",
       " 'knoow': 493,\n",
       " '(;': 494,\n",
       " 'request': 495,\n",
       " 'cancel': 496,\n",
       " 'account': 497,\n",
       " 'told': 498,\n",
       " '1-800-': 499,\n",
       " 'comcast-suck': 500,\n",
       " 'analyst': 501,\n",
       " 'left': 502,\n",
       " 'room': 503,\n",
       " 'keep': 504,\n",
       " 'insid': 505,\n",
       " 'almost': 506,\n",
       " 'alway': 507,\n",
       " 'ummm': 508,\n",
       " 'anonym': 509,\n",
       " 'yup': 510,\n",
       " 'fat': 511,\n",
       " 'albert': 512,\n",
       " 'next': 513,\n",
       " 'moffit': 514,\n",
       " 'cancer': 515,\n",
       " 'cntr': 516,\n",
       " 'pull': 517,\n",
       " 'valet': 518,\n",
       " 'park': 519,\n",
       " 'rais': 520,\n",
       " 'bracelet': 521,\n",
       " 'beat': 522,\n",
       " 'perspctv': 523,\n",
       " 'tnx': 524,\n",
       " 'relationship': 525,\n",
       " 'scare': 526,\n",
       " 'anyon': 527,\n",
       " 'snuggl': 528,\n",
       " 'kick': 529,\n",
       " 'miss': 530,\n",
       " 'jen': 531,\n",
       " 'mad': 532,\n",
       " 'reason': 533,\n",
       " 'burritovil': 534,\n",
       " 'read': 535,\n",
       " 'magazin': 536,\n",
       " 'rest': 537,\n",
       " 'bryan': 538,\n",
       " 'uglyyi': 539,\n",
       " 'dum': 540,\n",
       " 'hoe': 541,\n",
       " '5/19': 542,\n",
       " '10': 543,\n",
       " 'aha': 544,\n",
       " 'ugli': 545,\n",
       " 'coupl': 546,\n",
       " 'lovin': 547,\n",
       " 'dick': 548,\n",
       " 'chees': 549,\n",
       " 'alot': 550,\n",
       " 'probabl': 551,\n",
       " 'somewher': 552,\n",
       " 'ili': 553,\n",
       " 'mm': 554,\n",
       " 'lee': 555,\n",
       " 'fight': 556,\n",
       " 'dirti': 557,\n",
       " 'tell': 558,\n",
       " 'look': 559,\n",
       " 'colleg': 560,\n",
       " 'graduat': 561,\n",
       " 'major': 562,\n",
       " 'loser': 563,\n",
       " 'seth': 564,\n",
       " 'male': 565,\n",
       " 'model': 566,\n",
       " 'chicken': 567,\n",
       " '33': 568,\n",
       " 'serioulsi': 569,\n",
       " 'point': 570,\n",
       " 'fast': 571,\n",
       " 'finsh': 572,\n",
       " 'nope': 573,\n",
       " 'game': 574,\n",
       " 'straight': 575,\n",
       " 'momma': 576,\n",
       " 'kiss': 577,\n",
       " 'sweater': 578,\n",
       " 'o_o': 579,\n",
       " 'sorri': 580,\n",
       " 'didnt': 581,\n",
       " 'last': 582,\n",
       " 'night': 583,\n",
       " 'half': 584,\n",
       " 'asleep': 585,\n",
       " 'ur': 586,\n",
       " 'swell': 587,\n",
       " 'sir': 588,\n",
       " 'big': 589,\n",
       " 'pet': 590,\n",
       " 'cat': 591,\n",
       " 'adam': 592,\n",
       " 'wrong': 593,\n",
       " 'hmmm': 594,\n",
       " 'idk': 595,\n",
       " 'understand': 596,\n",
       " 'hahaa': 597,\n",
       " 'true': 598,\n",
       " 'condens': 599,\n",
       " 'windex': 600,\n",
       " 'greek': 601,\n",
       " 'wed': 602,\n",
       " 'yess': 603,\n",
       " 'unfortun': 604,\n",
       " 'oppos': 605,\n",
       " 'non': 606,\n",
       " 'stand': 607,\n",
       " 'cock': 608,\n",
       " 'gun': 609,\n",
       " 'dave': 610,\n",
       " 'damn-it': 611,\n",
       " 'power': 612,\n",
       " 'camera': 613,\n",
       " 'lmao': 614,\n",
       " 'mari': 615,\n",
       " 'hahahaha': 616,\n",
       " 'hate-fest': 617,\n",
       " 'way': 618,\n",
       " 'upset': 619,\n",
       " 'irl': 620,\n",
       " 'tie': 621,\n",
       " 'bed': 622,\n",
       " 'davro': 623,\n",
       " 'attent': 624,\n",
       " 'better': 625,\n",
       " 'thu': 626,\n",
       " 'whole': 627,\n",
       " 'dynam': 628,\n",
       " 'huh': 629,\n",
       " 'great': 630,\n",
       " 'show': 631,\n",
       " \"can't\": 632,\n",
       " 'wait': 633,\n",
       " 'vid': 634,\n",
       " 'pic': 635,\n",
       " 'super': 636,\n",
       " 'special': 637,\n",
       " 'sweet': 638,\n",
       " '9': 639,\n",
       " 'grad': 640,\n",
       " 'school': 641,\n",
       " 'agre': 642,\n",
       " 'ugo': 643,\n",
       " 'zerooo': 644,\n",
       " 'ping': 645,\n",
       " 'later': 646,\n",
       " 'sched': 647,\n",
       " 'write': 648,\n",
       " 'book': 649,\n",
       " 'digg': 650,\n",
       " 'nomin': 651,\n",
       " 'cuz': 652,\n",
       " \"he'\": 653,\n",
       " 'thee': 654,\n",
       " 'troll': 655,\n",
       " ';)': 656,\n",
       " 'nahh': 657,\n",
       " 'apolog': 658,\n",
       " 'overact': 659,\n",
       " 'hormon': 660,\n",
       " 'sometim': 661,\n",
       " 'contagi': 662,\n",
       " 'us': 663,\n",
       " 'cultur': 664,\n",
       " 'warp': 665,\n",
       " 'child': 666,\n",
       " 'viii': 667,\n",
       " 'excess': 668,\n",
       " 'bail': 669,\n",
       " 'cruel': 670,\n",
       " 'punish': 671,\n",
       " 'everyon': 672,\n",
       " 'goe': 673,\n",
       " 'law': 674,\n",
       " 'wrongli': 675,\n",
       " 'whether': 676,\n",
       " 'easili': 677,\n",
       " 'disgust': 678,\n",
       " 'govern': 679,\n",
       " 'fix': 680,\n",
       " 'enforc': 681,\n",
       " 'version': 682,\n",
       " 'beyonc': 683,\n",
       " 'follow': 684,\n",
       " 'rememb': 685,\n",
       " 'care': 686,\n",
       " 'omg': 687,\n",
       " 'fag': 688,\n",
       " 'eye': 689,\n",
       " 'drug': 690,\n",
       " 'are': 691,\n",
       " 'yhu': 692,\n",
       " 'inlove': 693,\n",
       " 'fool': 694,\n",
       " '247': 695,\n",
       " 'workhors': 696,\n",
       " 'die': 697,\n",
       " ':d': 698,\n",
       " 'king': 699,\n",
       " 'marri': 700,\n",
       " 'royal': 701,\n",
       " 'famili': 702,\n",
       " 'least': 703,\n",
       " 'okay': 704,\n",
       " 'sit': 705,\n",
       " 'psycho': 706,\n",
       " 'hahahah': 707,\n",
       " 'bastard': 708,\n",
       " 'problem': 709,\n",
       " 'im': 710,\n",
       " 'realllyyi': 711,\n",
       " 'forward': 712,\n",
       " 'mcr': 713,\n",
       " 'god': 714,\n",
       " 'met': 715,\n",
       " 'hahah': 716,\n",
       " 'friendship': 717,\n",
       " 'fake': 718,\n",
       " 'act': 719,\n",
       " 'lame': 720,\n",
       " 'ovious': 721,\n",
       " 'iloveyouhh': 722,\n",
       " 'more': 723,\n",
       " 'stop': 724,\n",
       " 'bleed': 725,\n",
       " 'awkward': 726,\n",
       " 'bring': 727,\n",
       " 'stuff': 728,\n",
       " 'boyfriend': 729,\n",
       " 'boo': 730,\n",
       " 'caus': 731,\n",
       " 'ive': 732,\n",
       " 'lay': 733,\n",
       " 'around': 734,\n",
       " 'hermet': 735,\n",
       " 'crab': 736,\n",
       " 'sick': 737,\n",
       " 'music': 738,\n",
       " 'tri': 739,\n",
       " 'tweet': 740,\n",
       " 'huck': 741,\n",
       " 'kid': 742,\n",
       " 'alright': 743,\n",
       " 'lololol': 744,\n",
       " 'happen': 745,\n",
       " 'idnt': 746,\n",
       " 'iguess': 747,\n",
       " 'bih': 748,\n",
       " 'sldnt': 749,\n",
       " 'cum': 750,\n",
       " 'mofo': 751,\n",
       " 'boxer': 752,\n",
       " 'n': 753,\n",
       " 'he': 754,\n",
       " 'guna': 755,\n",
       " 'b': 756,\n",
       " 'jumpin': 757,\n",
       " 'junk': 758,\n",
       " \"here'\": 759,\n",
       " 'teach': 760,\n",
       " 'properli': 761,\n",
       " 'parent': 762,\n",
       " 'wonder': 763,\n",
       " 'campu': 764,\n",
       " 'today': 765,\n",
       " 'nice': 766,\n",
       " 'sheen': 767,\n",
       " 'cover': 768,\n",
       " 'holi': 769,\n",
       " 'amaz': 770,\n",
       " 'retweet': 771,\n",
       " 'sucker': 772,\n",
       " 'absolut': 773,\n",
       " 'terribl': 774,\n",
       " 'w': 775,\n",
       " 'jocker': 776,\n",
       " 'deathcab': 777,\n",
       " 'cuti': 778,\n",
       " 'burn': 779,\n",
       " 'staff': 780,\n",
       " 'parti': 781,\n",
       " 'etc': 782,\n",
       " 'eep': 783,\n",
       " 'wine': 784,\n",
       " 'kim': 785,\n",
       " 'kardashian': 786,\n",
       " 'annoy': 787,\n",
       " 'reggi': 788,\n",
       " 'lilgego1youngg@yahoo.com': 789,\n",
       " 'kitchen': 790,\n",
       " 'qa': 791,\n",
       " 'far': 792,\n",
       " 'abduct': 793,\n",
       " 'alien': 794,\n",
       " 'anybodi': 795,\n",
       " 'heh': 796,\n",
       " 'thunk': 797,\n",
       " 'friggin': 798,\n",
       " 'daft': 799,\n",
       " 'anyway': 800,\n",
       " 'con': 801,\n",
       " 'whale': 802,\n",
       " 'doll': 803,\n",
       " 'brit': 804,\n",
       " 'wit': 805,\n",
       " 'hair': 806,\n",
       " 'leader': 807,\n",
       " 'previou': 808,\n",
       " 'content': 809,\n",
       " 'unfollow': 810,\n",
       " 'basic': 811,\n",
       " 'sell': 812,\n",
       " 'stranger': 813,\n",
       " \"must'v\": 814,\n",
       " 'freak': 815,\n",
       " 'talk': 816,\n",
       " 'smug': 817,\n",
       " 'cunt': 818,\n",
       " 'long-ass': 819,\n",
       " 'impress': 820,\n",
       " 'seen': 821,\n",
       " 'fail': 822,\n",
       " 'bomb': 823,\n",
       " 'build': 824,\n",
       " 'forum': 825,\n",
       " 'final': 826,\n",
       " 'cut': 827,\n",
       " 'documentari': 828,\n",
       " 'sex': 829,\n",
       " 'continu': 830,\n",
       " 'ladi': 831,\n",
       " 'comin': 832,\n",
       " 'frisco': 833,\n",
       " 'hunt': 834,\n",
       " 'close': 835,\n",
       " 'children': 836,\n",
       " 'sheila': 837,\n",
       " 'easton': 838,\n",
       " 'nah': 839,\n",
       " 'chick': 840,\n",
       " 'princ': 841,\n",
       " 'ugh': 842,\n",
       " 'brain-defici': 843,\n",
       " 'retard': 844,\n",
       " 'youtub': 845,\n",
       " 'gaia': 846,\n",
       " 'faggi': 847,\n",
       " 'grosss': 848,\n",
       " 'nasti': 849,\n",
       " 'offer': 850,\n",
       " 'fuck-ton': 851,\n",
       " 'beer': 852,\n",
       " 'fridg': 853,\n",
       " 'saturday': 854,\n",
       " 'suv': 855,\n",
       " 'monster': 856,\n",
       " 'k': 857,\n",
       " '8': 858,\n",
       " 'gig': 859,\n",
       " 'nearli': 860,\n",
       " 'terifi': 861,\n",
       " 'suspect': 862,\n",
       " 'gambit': 863,\n",
       " 'embrac': 864,\n",
       " 'spread': 865,\n",
       " 'finish': 866,\n",
       " 'questiosn': 867,\n",
       " 'pack': 868,\n",
       " 'leav': 869,\n",
       " 'thank': 870,\n",
       " 'hahahahah': 871,\n",
       " 'husband': 872,\n",
       " 'bit': 873,\n",
       " 'control': 874,\n",
       " 'hord': 875,\n",
       " 'allianc': 876,\n",
       " 'ilu': 877,\n",
       " 'how': 878,\n",
       " 'neck': 879,\n",
       " 'wood': 880,\n",
       " 'tv': 881,\n",
       " 'gener': 882,\n",
       " 'we': 883,\n",
       " 'constantli': 884,\n",
       " 'rage': 885,\n",
       " 'news': 886,\n",
       " 'current': 887,\n",
       " 'affair': 888,\n",
       " 'gett': 889,\n",
       " 'quit': 890,\n",
       " 'complain': 891,\n",
       " 'dress': 892,\n",
       " 'cold': 893,\n",
       " 'pick': 894,\n",
       " 'radish': 895,\n",
       " 'meat': 896,\n",
       " \"he'd\": 897,\n",
       " 'fine': 898,\n",
       " 'exactli': 899,\n",
       " 'pictur': 900,\n",
       " 'also': 901,\n",
       " 'bang': 902,\n",
       " 'join': 903,\n",
       " 'grief': 904,\n",
       " 'wink': 905,\n",
       " 'possibl': 906,\n",
       " 'tire': 907,\n",
       " 'nonsens': 908,\n",
       " 'blogger': 909,\n",
       " 'lem': 910,\n",
       " 'laneig': 911,\n",
       " 'whiten': 912,\n",
       " 'breakin': 913,\n",
       " 'somethin': 914,\n",
       " 'acn': 915,\n",
       " 'scar': 916,\n",
       " 'bleh': 917,\n",
       " 'yep': 918,\n",
       " 'hacker': 919,\n",
       " 'geek': 920,\n",
       " 'stay': 921,\n",
       " '@home': 922,\n",
       " 'dad': 923,\n",
       " 'move': 924,\n",
       " 'state': 925,\n",
       " 'dr': 926,\n",
       " 'embed': 927,\n",
       " 'link': 928,\n",
       " 'webkit': 929,\n",
       " 'pain': 930,\n",
       " 'sourc': 931,\n",
       " 'took': 932,\n",
       " '3': 933,\n",
       " 'error': 934,\n",
       " 'compil': 935,\n",
       " 'worki': 936,\n",
       " ':]': 937,\n",
       " 'weather': 938,\n",
       " 'appear': 939,\n",
       " 'boot': 940,\n",
       " 'travel': 941,\n",
       " \"holiday'\": 942,\n",
       " 'dude': 943,\n",
       " 'whore': 944,\n",
       " 'solar': 945,\n",
       " 'eclips': 946,\n",
       " '7': 947,\n",
       " 'year': 948,\n",
       " 'whatev': 949,\n",
       " 'gotten': 950,\n",
       " 'wear': 951,\n",
       " 'pant': 952,\n",
       " 'opaqu': 953,\n",
       " 'tight': 954,\n",
       " 'ran': 955,\n",
       " 'fire': 956,\n",
       " 'hydrant': 957,\n",
       " 'bruis': 958,\n",
       " '4eva': 959,\n",
       " 'coolest': 960,\n",
       " 'rotflmaooo': 961,\n",
       " 'cumleon': 962,\n",
       " 'that': 963,\n",
       " 'sloth': 964,\n",
       " 'came': 965,\n",
       " 'dragon': 966,\n",
       " 'wy': 967,\n",
       " 'thi': 968,\n",
       " 'wayyy': 969,\n",
       " 'energi': 970,\n",
       " 'lose': 971,\n",
       " 'weight': 972,\n",
       " 'aka': 973,\n",
       " 'science-bas': 974,\n",
       " 'composit': 975,\n",
       " 'patent': 976,\n",
       " 'double-blind': 977,\n",
       " 'studi': 978,\n",
       " 'hairi': 979,\n",
       " 'wtf': 980,\n",
       " 'laser': 981,\n",
       " 'ill': 982,\n",
       " 'messag': 983,\n",
       " 'spam': 984,\n",
       " 'igot': 985,\n",
       " 'yo': 986,\n",
       " 'lil': 987,\n",
       " 'behind': 988,\n",
       " 'serious': 989,\n",
       " 'air': 990,\n",
       " 'award': 991,\n",
       " 'interview': 992,\n",
       " \"mommy'\": 993,\n",
       " 'cheerlead': 994,\n",
       " 'welcom': 995,\n",
       " 'suree': 996,\n",
       " 'pig': 997,\n",
       " 'plenti': 998,\n",
       " 'space': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in X_train: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ae973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624e4418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " loves 1 Guy 1 Cup! She just admitted her lust for it! (@kristinaking ISO man who breaks glass in ass)\n",
      "\n",
      "Tensor of tweet:\n",
      " [200, 312, 447, 312, 248, 192, 7045, 12134, 180, 1461, 2881, 27]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", X_train[11284])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(X_train[11284], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce01fb31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8b23ad2f13fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" Tests passed out of 3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtest_tweet_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-8b23ad2f13fe>\u001b[0m in \u001b[0;36mtest_tweet_to_tensor\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         {\n\u001b[0;32m      5\u001b[0m             \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"simple_test_check\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[1;34m\"input\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVocab\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;34m\"expected\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m444\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m304\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m567\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m56\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;34m\"error\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"The function gives bad output for val_pos[1]. Test failed\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_pos' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013d64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34d0f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 1.421e+03 1.425e+03]]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = extract_features(X_train[11284], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8087f145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.zeros((len(X_train),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3063fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i in X_train:\n",
    "    x[j,:]=extract_features(i,freqs)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e35e91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09632519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "72508541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([x[5,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51626dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs):\n",
    "    x = extract_features(tweet,freqs)\n",
    "    y_pred = clf.predict(x)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ab4c6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:05<00:00, 689.78it/s]\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in tqdm(X_test):\n",
    "    result.append(predict_tweet(i,freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de33dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction=0\n",
    "wrong_prediction=0\n",
    "for i in range(4000):\n",
    "    if result[i]==y_teet[i]:\n",
    "        correct_prediction+=1\n",
    "    else:\n",
    "        wrong_prediction+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d7c5513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2620"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eaf1c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6565842f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4000-2620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "705ff997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1320/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8c9dd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2620/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4fde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
